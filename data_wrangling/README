File Structure:
resources - this folder contains the dictionaries for normalizing the tweet texts. These are Lexical normalisation dictionaries: UniMelb, UTDallas taken from https://noisy-text.github.io/norm-shared-task.html. It also contains the pretrained GoogleWord2Vec model.
main.py - driver file
preprocess.py - preprocess all of the data to get rid of junk chars in the input.
topicmodel.py - contains wrapper functions to call the gensim LDA model.
word2vec_cosine.py - contains all the code for getting word2vec features.
README - this file


All code is in python 2, and requires gensim, nltk, pandas and scikit-learn and the file GoogleNews-vectors-negative300.bin.gz in the resources folder.

To run the code:

Just call main.py and the user is prompted to choose number of topics, and number of words per topic.

Brief implementation details:
Our Tweet Topic Modeling and Emotion Analysis occurs in the following steps (and corresponding files mentioned therein):

1. Preprocessing and Text Normalization: This is mainly done in the preprocess.py

The preprocess() function does the following:
	a) remove html tags and URLs by regex matching.
	b) remove @ from mentions, also by regex matching
	c) remove trailing hashtags by simply reading the tweet tokens backwards and checking if they begin with the # sign, until a token is encountered that doesn't begin with a # when the loop breaks, and the remainder of the tokens are returned.
	d) normalize text. This is done using Lexical normalisation dictionaries: UniMelb, UTDallas taken from https://noisy-text.github.io/norm-shared-task.html
	e) remove stopwords
	f) normalize twitter text.

2. Getting emotions: We automatically labelled each tweet with one of the 6 emotions: joy, sadness, anger, guilt, disgust and fear. We did this by finding the word2vec representation of the tweet from a pre-trained GoogleNGram model (can be downloaded here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). It is a 1.5 GB file. Next we found the cosine distance of the vector representation of each tweet with the vector representation of each emotion, and labeled it with the emotion that had the smallest distance This happens in the funtion get_emotion() in preprocess.py. All the word2vec code is in a file called word2vec_cosine.py


3. Topic Modeling: This is done in topicmodel.py. This calls the gensim LDA topic model. There is a stemAndTokenize() function that performs stemming (Porter stemmer) and tokenization using NLTK. There are also functions to write the results to files in topicmodel.py.

4. main.py:
The main file performs the above steps by sequentially calling the above functions to produce the result files.

Resultant files are produced at the above steps, such as womeninstem-preprocessed.csv and womeninstem-lda-k5-n10.txt (where lda is the model name, k = 5 is the number of topics, and n=10 is the number of words per topic) womeninstem-emotions.csv



